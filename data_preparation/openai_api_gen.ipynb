{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oVy155VxmrSF",
    "outputId": "95e0bd6b-dbd3-4df9-849e-a06de6b869e1"
   },
   "outputs": [],
   "source": [
    "# !pip install openai\n",
    "# !pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aq_96oYnwO-_",
    "outputId": "a1dd6457-7245-40ef-d425-2cc73be8b3db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 3547350\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "\n",
    "with open('train_processed.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model('gpt-4')\n",
    "\n",
    "# Calculate the total number of tokens for each entry\n",
    "def calculate_tokens(entry):\n",
    "    text = f\"Category: {entry['category_name']}\\nTitle: {entry['title']}\\nDescription: {entry['description']}\\nAttributes: {entry['attributes']}\\nPrice: {entry['price']}\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n",
    "total_tokens = sum(calculate_tokens(entry) for entry in data)\n",
    "print(f\"Total tokens: {total_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UBng84ZsR-Hb"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model('gpt-4')\n",
    "\n",
    "def calculate_tokens(entry):\n",
    "    text = (f\"Category: {entry['category_name']}\\n\"\n",
    "            f\"Title: {entry['title']}\\n\"\n",
    "            f\"Description: {entry['description']}\\n\"\n",
    "            f\"Attributes: {entry['attributes']}\\n\"\n",
    "            f\"Price: {entry['price']}\")\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def get_question_count(percentile):\n",
    "    return int((percentile-0.000001)/10) + 5\n",
    "\n",
    "def calculate_percentiles(data):\n",
    "    token_counts = [calculate_tokens(entry) for entry in data]\n",
    "    sorted_token_counts = np.sort(token_counts)\n",
    "    return token_counts, sorted_token_counts\n",
    "\n",
    "def assign_questions_based_on_tokens(entry, sorted_token_counts, total_entries):\n",
    "    token_count = calculate_tokens(entry)\n",
    "    percentile = np.searchsorted(sorted_token_counts, token_count) / total_entries * 100\n",
    "    return get_question_count(percentile)\n",
    "\n",
    "token_counts, sorted_token_counts = calculate_percentiles(data)\n",
    "total_entries = len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3pMye5-9xpi3"
   },
   "outputs": [],
   "source": [
    "def preprocess_description(description):\n",
    "    return description.replace(\"[NEWLINE]\", \"\\n\").strip()\n",
    "def create_prompt(entry, questions_num):\n",
    "    prompt = (\n",
    "        f\"На основе следующего объявления сгенерируйте {questions_num} вопросов и соответствующих ответов.\"\n",
    "        f\"Придумывая вопрос, представьте, что вы играете роль покупателя, который заинтересован в товаре и \"\n",
    "        f\"хочет узнать как можно больше, а также, совершить выгодную сделку.\"\n",
    "        f\"Придумывайте вопросы, которые могли бы задать реальные покупатели, интересующиеся покупкой по этому объявлению. \"\n",
    "        f\"Придумав вопрос, ответьте на него, как если бы вы были экспертом по этому товару. \"\n",
    "        f\"Отвечая на вопрос, используйте только ту информацию о товаре, которая доступна в объявлении, \"\n",
    "        f\"либо же является общеизвестным фактом. \"\n",
    "\n",
    "        f\"**Обратите внимание**:\\n\"\n",
    "        f\"- Вопросы должны быть разной длины и сложности, имитируя реальные вопросы пользователей.\\n\"\n",
    "        f\"- Ответы должны быть краткими и точными.\\n\"\n",
    "        f\"- Формат ответа должен быть строгим: 'Вопрос: {{вопрос}}\\n', после вопроса - 'Ответ: {{ответ}}'.\\n\"\n",
    "        f\"- Если невозможно дать ответ на поставленный вопрос, используя информацию о товаре из объявления, \"\n",
    "        f\"либо же используя общеизвестные факты, то ответьте 'Пожалуйста, уточните у продавца'\"\n",
    "\n",
    "        f\"Далее идет информация об объявлении:\\n\"\n",
    "        f\"**Объявление**:\\n\"\n",
    "        f\"Категория: {entry['category_name']}\\n\"\n",
    "        f\"Заголовок: {entry['title']}\\n\"\n",
    "        f\"Описание: {preprocess_description(entry['description'])}\\n\"\n",
    "        f\"Характеристики: {entry['attributes']}\\n\"\n",
    "        f\"Цена: {entry['price']} руб.\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# with open('processed_data.json', 'r', encoding='utf-8') as file:\n",
    "#     data = json.load(file)\n",
    "# prompts = [create_prompt(entry, assign_questions_based_on_tokens(entry, sorted_token_counts, total_entries))\\\n",
    "#            for entry in data]\n",
    "# print(prompts[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p1xTpQ7Yq7Dw"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "api_key = \"YOUR-API-KEY\"\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nuoUkwHft40A",
    "outputId": "1be65543-8df6-437e-e60f-9c8554a47362"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5619\n",
      "5639\n",
      "5659\n",
      "5679\n",
      "save, i = 5699\n",
      "5719\n",
      "5739\n",
      "5759\n",
      "5779\n",
      "save, i = 5799\n",
      "5819\n",
      "5839\n",
      "5859\n",
      "5879\n",
      "save, i = 5899\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def parse_qa_pairs(model_output):\n",
    "    qa_pattern = re.compile(r\"Вопрос:\\s*(.*?)\\s*Ответ:\\s*(.*?)($|\\n)\", re.DOTALL)\n",
    "    qa_pairs = [{\"question\": q.strip(), \"answer\": a.strip()} for q, a, _ in qa_pattern.findall(model_output)]\n",
    "\n",
    "    return qa_pairs\n",
    "\n",
    "def generate_qa(entry, client):\n",
    "    prompt = create_prompt(entry, assign_questions_based_on_tokens(entry, sorted_token_counts, total_entries))\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Вы - агент поддержки клиентов для платформы Avito. Avito - это платформа \"\n",
    "         \"для размещения объявлений о продаже товаров и услуг. Ваша задача — помогать клиентам, генерируя \"\n",
    "         \"релевантные вопросы и предоставляя точные ответы на основе объявления. Вы должны отвечать чётко и \"\n",
    "         \"вежливо, имитируя реальные вопросы и ответы от пользователей платформы.\"\n",
    "\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "      ]\n",
    "    )\n",
    "    model_output = response.choices[0].message.content\n",
    "\n",
    "    lines = model_output.split(\"\\n\")\n",
    "    qa_pairs = parse_qa_pairs(model_output)\n",
    "\n",
    "    result = {\n",
    "        \"category\": entry['category_name'],\n",
    "        \"title\": entry['title'],\n",
    "        \"description\": preprocess_description(entry['description']),\n",
    "        \"attributes\": entry['attributes'],\n",
    "        \"price\": entry['price'],\n",
    "        \"generated_questions\": qa_pairs\n",
    "    }\n",
    "\n",
    "    return result, model_output\n",
    "\n",
    "\n",
    "# res, model_out= generate_qa(data[12], client)\n",
    "\n",
    "data_qa = []\n",
    "def save(data_qa):\n",
    "  with open('data_qa.json', 'w', encoding='utf-8') as outfile:\n",
    "      json.dump(data_qa, outfile, indent=4, ensure_ascii=False)\n",
    "\n",
    "for i, entry in enumerate(data):\n",
    "    generated_entry, model_out = generate_qa(entry, client)\n",
    "    data_qa.append(generated_entry)\n",
    "    if (i+1) % 20 == 0 and (i+1)%100 != 0:\n",
    "      print(i)\n",
    "    if (i+1)%100 == 0:\n",
    "      print(f'save, i = {i}')\n",
    "      save(data_qa)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7A58Da7amjSg",
    "outputId": "0ad5451f-6c3a-40bf-da7f-0c367d7fa3b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **Вопросы по теме объявления, на которые можно дать точный ответ:**\n",
      "\n",
      "   Вопрос: Какова диагональ экрана этого монитора?\n",
      "   Ответ: Диагональ экрана составляет 18.5 дюймов (47 см).\n",
      "\n",
      "   Вопрос: Какое разрешение у монитора Philips 191V2SB/6?\n",
      "   Ответ: Разрешение монитора составляет 1366x768 пикселей.\n",
      "\n",
      "   Вопрос: Входит ли кабель VGA в комплектацию?\n",
      "   Ответ: Да, кабель VGA входит в комплектацию монитора.\n",
      "\n",
      "   Вопрос: Какой угол обзора у этого монитора?\n",
      "   Ответ: Угол обзора монитора составляет 170 градусов по вертикали и 176 градусов по горизонтали.\n",
      "\n",
      "   Вопрос: Где находится магазин, в котором продается этот монитор?\n",
      "   Ответ: Магазин находится в Нижнем Новгороде, по адресу: ул. Чаадаева, д. 26.\n",
      "\n",
      "2. **Вопросы по теме объявления, на которые нельзя дать точный ответ:**\n",
      "\n",
      "   Вопрос: Есть ли у монитора какие-либо внешние повреждения или царапины?\n",
      "   Ответ: Эту информацию следует уточнить у продавца.\n",
      "\n",
      "3. **Вопросы, которые не относятся к теме объявления:**\n",
      "\n",
      "   Вопрос: Можете порекомендовать лучший монитор для создания графики?\n",
      "   Ответ: Вопрос не относится к теме объявления.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Вы - агент поддержки клиентов для платформы Avito. Avito - это платформа \"\n",
    "         \"для размещения объявлений о продаже товаров и услуг. Ваша задача — помогать клиентам, генерируя \"\n",
    "         \"релевантные вопросы и предоставляя точные ответы на основе объявления. Вы должны отвечать чётко и \"\n",
    "         \"вежливо, имитируя реальные вопросы и ответы от пользователей платформы.\"\n",
    "\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompts[0]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заметки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оказалось, что модель плохо моделирует вопросы, не относящиеся к теме объявления. Практически все такие вопросы на самом деле относятся к теме объявления, и скорее, должны иметь метку \"Эту информацию следует уточнить у продавца\".   \n",
    "Пример:    \n",
    "'question': 'Вопрос: Есть ли гарантия на товары, которые вы продаете?',    \n",
    "'answer': 'Ответ: Вопрос не относится к теме объявления.'   \n",
    "    \n",
    "Поэтому, не будем генерировать такие вопросы. Можно оставить задачу генерации вопросов, не относящихся к теме объявления, другой модели (классификация с BERT, например)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также, после некоторых экспериментов, был изменен промпт. Из промпта убраны указания о том, сколько % вопросов должны быть релевантными (по теме и тексту объявления), сколько нерелевантными, сколько релевантными, но не имеющими ответа. Основные причины:    \n",
    "1. Это вынуждает модель больше фокусироваться на структуре, чем на смысле задаваемых вопросов. Это существенно снижает их качество.  \n",
    "2. Вопросы с ответом \"Пожалуйста, уточните у продавца\" с новым промптом генерируются тогда, когда действительно являются уместными."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
